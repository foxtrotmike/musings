{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXy3DesxbCDIKoK02JzOzD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foxtrotmike/musings/blob/main/vtransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mNyVjy9pZJp",
        "outputId": "e8108b9c-d03d-43a7-ebe3-ba9e8199b7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    patient_id  time  measurement_1  measurement_2  measurement_3\n",
            "0            1     6      28.214488      25.393803       1.788428\n",
            "1            1    21       2.575569      57.769994      46.926204\n",
            "2            1    52      43.186728      63.535987      11.080344\n",
            "3            1    59      78.745628      45.087964      35.086413\n",
            "4            1    63      21.947808      96.519428      11.337316\n",
            "5            1    67      33.094361      72.345273      62.391860\n",
            "6            2     7      77.289804      56.127689      75.777841\n",
            "7            2    28       0.466625      92.337506       6.906353\n",
            "8            2    29      27.838122      65.696205      39.141420\n",
            "9            2    33       7.986671      88.166573      86.680852\n",
            "10           2    50      25.640740      56.371510       8.314938\n",
            "11           2    55      93.037869      91.857094      33.617259\n",
            "12           2    97      38.875356      98.153366      51.192967\n",
            "13           3     5      81.554330      25.355395      99.984621\n",
            "14           3    14      18.936020      37.707560      62.573979\n",
            "15           3    19      98.845281      58.035113       2.774941\n",
            "16           3    22      23.866479      71.217415      29.831619\n",
            "17           3    29      41.441233      49.098396      52.410434\n",
            "18           3    32      69.263047      46.014296       2.916859\n",
            "19           3    38      51.317249      73.367713      90.429362\n",
            "20           3    41      24.083993      88.834607      99.884742\n",
            "21           3    46      97.336267      49.857079      59.236214\n",
            "22           3    50      78.479477       4.028026      94.966723\n",
            "23           3    56      70.232428      25.590431      34.325201\n",
            "24           3    58      43.501153      42.495433       5.847056\n",
            "25           3    60      52.828022      54.688415      93.574136\n",
            "26           3    75      49.301753      79.986972      60.223580\n",
            "27           3    78      65.159769      25.772925      67.935928\n",
            "28           3    80      79.846691      41.391798      70.016983\n",
            "29           3    81      28.935335      70.151465      22.074175\n",
            "30           3    91       2.838407      54.046115      70.077446\n",
            "31           3    96      30.282225      79.487241      37.250963\n",
            "32           3    99      57.433531      74.987259      90.861779\n",
            "33           4    43      20.835626      36.591226      49.898133\n",
            "34           4    46      54.923673      39.266359      53.795927\n",
            "35           4    48       4.164962      86.082422      32.608439\n",
            "36           4    66      38.985897      42.066495      92.818190\n",
            "37           4    70       5.081666      18.864361      69.927642\n",
            "38           4    77      18.954667      86.319439      65.916052\n",
            "39           4    83      16.572509      99.247077      40.911948\n",
            "40           4    90      61.270739      78.209595      70.175828\n",
            "41           4    94      14.608556      52.642962      81.770362\n",
            "42           5     1      58.015609      35.524663      60.329252\n",
            "43           5     2      10.359705      82.560635      76.365581\n",
            "44           5     4      60.060221      49.880250      12.572850\n",
            "45           5    15      82.164339      66.499525      41.348268\n",
            "46           5    18      32.040839      80.363199      94.213264\n",
            "47           5    19      72.994779      83.076598      96.470052\n",
            "48           5    31       1.576901      61.071065      41.650300\n",
            "49           5    32      17.766341      32.628900      38.436926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7607957720756531\n",
            "Epoch 2, Loss: 0.17592217028141022\n",
            "Epoch 3, Loss: 0.16767065227031708\n",
            "Epoch 4, Loss: 0.15558499097824097\n",
            "Epoch 5, Loss: 0.1413862258195877\n",
            "Epoch 6, Loss: 0.1255502998828888\n",
            "Epoch 7, Loss: 0.11852536350488663\n",
            "Epoch 8, Loss: 0.11318311095237732\n",
            "Epoch 9, Loss: 0.10218221694231033\n",
            "Epoch 10, Loss: 0.09718501567840576\n",
            "Epoch 11, Loss: 0.09024440497159958\n",
            "Epoch 12, Loss: 0.08566103130578995\n",
            "Epoch 13, Loss: 0.08285683393478394\n",
            "Epoch 14, Loss: 0.08484569936990738\n",
            "Epoch 15, Loss: 0.08462855219841003\n",
            "Epoch 16, Loss: 0.08179233223199844\n",
            "Epoch 17, Loss: 0.08445437997579575\n",
            "Epoch 18, Loss: 0.08609229326248169\n",
            "Epoch 19, Loss: 0.08584374189376831\n",
            "Epoch 20, Loss: 0.08617661148309708\n",
            "Epoch 21, Loss: 0.08712138235569\n",
            "Epoch 22, Loss: 0.08769042044878006\n",
            "Epoch 23, Loss: 0.08646060526371002\n",
            "Epoch 24, Loss: 0.08669541776180267\n",
            "Epoch 25, Loss: 0.08639346808195114\n",
            "Epoch 26, Loss: 0.08558256179094315\n",
            "Epoch 27, Loss: 0.08509645611047745\n",
            "Epoch 28, Loss: 0.08375903964042664\n",
            "Epoch 29, Loss: 0.08276286721229553\n",
            "Epoch 30, Loss: 0.08291807025671005\n",
            "Epoch 31, Loss: 0.08375532180070877\n",
            "Epoch 32, Loss: 0.08390440791845322\n",
            "Epoch 33, Loss: 0.08224673569202423\n",
            "Epoch 34, Loss: 0.079290471971035\n",
            "Epoch 35, Loss: 0.07974906265735626\n",
            "Epoch 36, Loss: 0.08211063593626022\n",
            "Epoch 37, Loss: 0.07941728085279465\n",
            "Epoch 38, Loss: 0.08016125112771988\n",
            "Epoch 39, Loss: 0.07942068576812744\n",
            "Epoch 40, Loss: 0.08004479855298996\n",
            "Epoch 41, Loss: 0.08343913406133652\n",
            "Epoch 42, Loss: 0.07995077967643738\n",
            "Epoch 43, Loss: 0.08241678774356842\n",
            "Epoch 44, Loss: 0.0801619291305542\n",
            "Epoch 45, Loss: 0.08108564466238022\n",
            "Epoch 46, Loss: 0.08029323071241379\n",
            "Epoch 47, Loss: 0.08046666532754898\n",
            "Epoch 48, Loss: 0.07999812066555023\n",
            "Epoch 49, Loss: 0.080605648458004\n",
            "Epoch 50, Loss: 0.08000046014785767\n",
            "Epoch 51, Loss: 0.07983605563640594\n",
            "Epoch 52, Loss: 0.0795930027961731\n",
            "Epoch 53, Loss: 0.07941898703575134\n",
            "Epoch 54, Loss: 0.07954712957143784\n",
            "Epoch 55, Loss: 0.0797179564833641\n",
            "Epoch 56, Loss: 0.07916927337646484\n",
            "Epoch 57, Loss: 0.08051202446222305\n",
            "Epoch 58, Loss: 0.07891323417425156\n",
            "Epoch 59, Loss: 0.07935628294944763\n",
            "Epoch 60, Loss: 0.08167286217212677\n",
            "Epoch 61, Loss: 0.07827483862638474\n",
            "Epoch 62, Loss: 0.08009565621614456\n",
            "Epoch 63, Loss: 0.07929719239473343\n",
            "Epoch 64, Loss: 0.08138228952884674\n",
            "Epoch 65, Loss: 0.08194173127412796\n",
            "Epoch 66, Loss: 0.07866671681404114\n",
            "Epoch 67, Loss: 0.07986459881067276\n",
            "Epoch 68, Loss: 0.07923708111047745\n",
            "Epoch 69, Loss: 0.07891417294740677\n",
            "Epoch 70, Loss: 0.07935699820518494\n",
            "Epoch 71, Loss: 0.07977432012557983\n",
            "Epoch 72, Loss: 0.07892615348100662\n",
            "Epoch 73, Loss: 0.07984640449285507\n",
            "Epoch 74, Loss: 0.07883624732494354\n",
            "Epoch 75, Loss: 0.07955100387334824\n",
            "Epoch 76, Loss: 0.07938267290592194\n",
            "Epoch 77, Loss: 0.08032218366861343\n",
            "Epoch 78, Loss: 0.07834909856319427\n",
            "Epoch 79, Loss: 0.07933702319860458\n",
            "Epoch 80, Loss: 0.0786629468202591\n",
            "Epoch 81, Loss: 0.07893802225589752\n",
            "Epoch 82, Loss: 0.0787167102098465\n",
            "Epoch 83, Loss: 0.0788487121462822\n",
            "Epoch 84, Loss: 0.07940441370010376\n",
            "Epoch 85, Loss: 0.07894478738307953\n",
            "Epoch 86, Loss: 0.078830786049366\n",
            "Epoch 87, Loss: 0.07804976403713226\n",
            "Epoch 88, Loss: 0.07916639745235443\n",
            "Epoch 89, Loss: 0.07809028774499893\n",
            "Epoch 90, Loss: 0.07913520187139511\n",
            "Epoch 91, Loss: 0.0793355405330658\n",
            "Epoch 92, Loss: 0.07888665795326233\n",
            "Epoch 93, Loss: 0.07846444100141525\n",
            "Epoch 94, Loss: 0.07894747704267502\n",
            "Epoch 95, Loss: 0.07904430478811264\n",
            "Epoch 96, Loss: 0.07926423102617264\n",
            "Epoch 97, Loss: 0.07923931628465652\n",
            "Epoch 98, Loss: 0.07878793776035309\n",
            "Epoch 99, Loss: 0.0800187960267067\n",
            "Epoch 100, Loss: 0.07956351339817047\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_biomedical_data(num_patients, max_time_points, measurement_dim):\n",
        "    # Initialize a list to store data\n",
        "    data = []\n",
        "\n",
        "    # Generate data for each patient\n",
        "    for patient_id in range(1, num_patients + 1):\n",
        "        # Generate random number of time points for each patient\n",
        "        num_time_points = np.random.randint(5, max_time_points + 1)\n",
        "\n",
        "        # Generate unevenly spaced time points\n",
        "        times = np.sort(np.random.choice(range(1, 100), size=num_time_points, replace=False))\n",
        "\n",
        "        # Generate measurements for each time point\n",
        "        measurements = np.random.rand(num_time_points, measurement_dim) * 100\n",
        "\n",
        "        # Create data for each time point\n",
        "        for time, measurement in zip(times, measurements):\n",
        "            data.append([patient_id, time, *measurement])\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    column_names = ['patient_id', 'time'] + [f'measurement_{i+1}' for i in range(measurement_dim)]\n",
        "    df = pd.DataFrame(data, columns=column_names)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Generate a toy dataset\n",
        "num_patients = 10\n",
        "max_time_points = 20\n",
        "measurement_dim = 3  # For example, could represent different types of biomedical readings\n",
        "df = generate_biomedical_data(num_patients, max_time_points, measurement_dim)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head(50))\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, feature_size, num_layers, num_heads, dropout_rate=0.1):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=feature_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dropout=dropout_rate,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.output_layer = nn.Linear(feature_size, feature_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        # src and tgt are the input and target sequences\n",
        "        # src_mask and tgt_mask are the padding and causal masks respectively\n",
        "        out = self.transformer(src, tgt, src_key_padding_mask=src_mask, tgt_mask=tgt_mask, memory_key_padding_mask=src_mask)\n",
        "        return self.output_layer(out)\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz, device=src.device)).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "#def train(transformer, data_loader, loss_fn, optimizer, epochs=10):\n",
        "\n",
        "\n",
        "# Example use case: Assume `data_loader` is your PyTorch DataLoader that yields batches of data.\n",
        "# This is a minimalistic and illustrative example. In practice, you'll need to define the dataset class,\n",
        "# and handle batching, and possibly GPU computations.\n",
        "feature_size = 3  # Same as measurement dimensions\n",
        "num_layers = 3\n",
        "num_heads = 3\n",
        "transformer = TimeSeriesTransformer(feature_size, num_layers, num_heads)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Mock data loader (replace this with actual data preparation)\n",
        "src = torch.rand(10, 20, feature_size)  # (batch_size, sequence_length, feature_size)\n",
        "tgt = torch.rand(10, 20, feature_size)  # Same shape as src for simplicity\n",
        "data_loader = [(src, tgt)]\n",
        "\n",
        "# Train the model\n",
        "transformer.train()\n",
        "for epoch in range(100):\n",
        "    for src, tgt in data_loader:\n",
        "        src_mask = None  # No source masking in this autoregressive task\n",
        "        tgt_input = tgt[:, :-1]  # Use all but the last token for input to predict the next token\n",
        "        tgt_output = tgt[:, 1:]  # Use all but the first token for the target output\n",
        "        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1))  # Mask for the input part\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = transformer(src, tgt_input, src_mask, tgt_mask)\n",
        "        loss = loss_fn(output, tgt_output)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lGtG1B-UpZrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}