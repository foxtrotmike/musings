{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2nmbvUoZ94+B4/eyHnMeE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foxtrotmike/musings/blob/main/vtransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mNyVjy9pZJp",
        "outputId": "76128c40-ac14-414a-faaa-ca293846f75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    patient_id  time  measurement_1  measurement_2  measurement_3\n",
            "0            1     3       0.650970       0.628381       0.591015\n",
            "1            1     4       1.650970       2.256761       2.773046\n",
            "2            1     6       3.650970       6.513522      10.319137\n",
            "3            1    12       9.650970      19.027045      36.957412\n",
            "4            1    15      12.650970      41.054090     113.872235\n",
            "5            1    18      15.650970      85.108180     344.616704\n",
            "6            2     7       0.561790       0.492265       0.254079\n",
            "7            2     8       1.561790       1.984531       1.762236\n",
            "8            2    13       6.561790       8.969061      10.286709\n",
            "9            2    14       7.561790      18.938123      31.860126\n",
            "10           2    19      12.561790      42.876245     100.580379\n",
            "11           3     4       0.743339       0.254386       0.654445\n",
            "12           3    10       6.743339       6.508771       7.963336\n",
            "13           3    17      13.743339      20.017543      30.890008\n",
            "14           3    24      20.743339      47.035086      99.670023\n",
            "Epoch 1, Loss: 1.1367369890213013\n",
            "Epoch 2, Loss: 0.7888035774230957\n",
            "Epoch 3, Loss: 0.6948723793029785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 0.5918954014778137\n",
            "Epoch 5, Loss: 0.49352073669433594\n",
            "Epoch 6, Loss: 0.4048081338405609\n",
            "Epoch 7, Loss: 0.3490794003009796\n",
            "Epoch 8, Loss: 0.3005492389202118\n",
            "Epoch 9, Loss: 0.26958638429641724\n",
            "Epoch 10, Loss: 0.23460052907466888\n",
            "Epoch 11, Loss: 0.21801765263080597\n",
            "Epoch 12, Loss: 0.19784103333950043\n",
            "Epoch 13, Loss: 0.19142590463161469\n",
            "Epoch 14, Loss: 0.18658648431301117\n",
            "Epoch 15, Loss: 0.1781204491853714\n",
            "Epoch 16, Loss: 0.1634501963853836\n",
            "Epoch 17, Loss: 0.1551261693239212\n",
            "Epoch 18, Loss: 0.1424313187599182\n",
            "Epoch 19, Loss: 0.13384288549423218\n",
            "Epoch 20, Loss: 0.1246180459856987\n",
            "Epoch 21, Loss: 0.1151435449719429\n",
            "Epoch 22, Loss: 0.11274189502000809\n",
            "Epoch 23, Loss: 0.103813037276268\n",
            "Epoch 24, Loss: 0.09800536930561066\n",
            "Epoch 25, Loss: 0.10061686486005783\n",
            "Epoch 26, Loss: 0.09155683219432831\n",
            "Epoch 27, Loss: 0.09469671547412872\n",
            "Epoch 28, Loss: 0.09001170098781586\n",
            "Epoch 29, Loss: 0.08556445688009262\n",
            "Epoch 30, Loss: 0.08876094222068787\n",
            "Epoch 31, Loss: 0.08997109532356262\n",
            "Epoch 32, Loss: 0.08714040368795395\n",
            "Epoch 33, Loss: 0.08877596259117126\n",
            "Epoch 34, Loss: 0.09288377314805984\n",
            "Epoch 35, Loss: 0.08954405039548874\n",
            "Epoch 36, Loss: 0.09066547453403473\n",
            "Epoch 37, Loss: 0.08921466022729874\n",
            "Epoch 38, Loss: 0.0908832848072052\n",
            "Epoch 39, Loss: 0.09380682557821274\n",
            "Epoch 40, Loss: 0.09076884388923645\n",
            "Epoch 41, Loss: 0.09306088835000992\n",
            "Epoch 42, Loss: 0.09226351231336594\n",
            "Epoch 43, Loss: 0.09234704077243805\n",
            "Epoch 44, Loss: 0.09237805753946304\n",
            "Epoch 45, Loss: 0.08974029123783112\n",
            "Epoch 46, Loss: 0.088324174284935\n",
            "Epoch 47, Loss: 0.09440415352582932\n",
            "Epoch 48, Loss: 0.08891726285219193\n",
            "Epoch 49, Loss: 0.08875526487827301\n",
            "Epoch 50, Loss: 0.09117992222309113\n",
            "Epoch 51, Loss: 0.09112606197595596\n",
            "Epoch 52, Loss: 0.08741699904203415\n",
            "Epoch 53, Loss: 0.09202730655670166\n",
            "Epoch 54, Loss: 0.08987013250589371\n",
            "Epoch 55, Loss: 0.08664141595363617\n",
            "Epoch 56, Loss: 0.08729245513677597\n",
            "Epoch 57, Loss: 0.08987893909215927\n",
            "Epoch 58, Loss: 0.0885731428861618\n",
            "Epoch 59, Loss: 0.09004785865545273\n",
            "Epoch 60, Loss: 0.08874506503343582\n",
            "Epoch 61, Loss: 0.0888606458902359\n",
            "Epoch 62, Loss: 0.08671213686466217\n",
            "Epoch 63, Loss: 0.08538932353258133\n",
            "Epoch 64, Loss: 0.08766541630029678\n",
            "Epoch 65, Loss: 0.08810145407915115\n",
            "Epoch 66, Loss: 0.08668508380651474\n",
            "Epoch 67, Loss: 0.08517242223024368\n",
            "Epoch 68, Loss: 0.08611177653074265\n",
            "Epoch 69, Loss: 0.0876830667257309\n",
            "Epoch 70, Loss: 0.0850306898355484\n",
            "Epoch 71, Loss: 0.0861923098564148\n",
            "Epoch 72, Loss: 0.08521084487438202\n",
            "Epoch 73, Loss: 0.08651591837406158\n",
            "Epoch 74, Loss: 0.08684923499822617\n",
            "Epoch 75, Loss: 0.08815446496009827\n",
            "Epoch 76, Loss: 0.08744700253009796\n",
            "Epoch 77, Loss: 0.08759573101997375\n",
            "Epoch 78, Loss: 0.08685585111379623\n",
            "Epoch 79, Loss: 0.08751656115055084\n",
            "Epoch 80, Loss: 0.08694881945848465\n",
            "Epoch 81, Loss: 0.08702220022678375\n",
            "Epoch 82, Loss: 0.08743510395288467\n",
            "Epoch 83, Loss: 0.0870710164308548\n",
            "Epoch 84, Loss: 0.08725204318761826\n",
            "Epoch 85, Loss: 0.08548153191804886\n",
            "Epoch 86, Loss: 0.08531798422336578\n",
            "Epoch 87, Loss: 0.08968781679868698\n",
            "Epoch 88, Loss: 0.08490337431430817\n",
            "Epoch 89, Loss: 0.08803144842386246\n",
            "Epoch 90, Loss: 0.08713622391223907\n",
            "Epoch 91, Loss: 0.08618305623531342\n",
            "Epoch 92, Loss: 0.08902556449174881\n",
            "Epoch 93, Loss: 0.08743054419755936\n",
            "Epoch 94, Loss: 0.0858701765537262\n",
            "Epoch 95, Loss: 0.0848674476146698\n",
            "Epoch 96, Loss: 0.08500868082046509\n",
            "Epoch 97, Loss: 0.08730977773666382\n",
            "Epoch 98, Loss: 0.08533830940723419\n",
            "Epoch 99, Loss: 0.08697468042373657\n",
            "Epoch 100, Loss: 0.08702444285154343\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_dynamic_data(num_patients, max_time_points, num_features):\n",
        "    data = []\n",
        "\n",
        "    # Generate data for each patient\n",
        "    for patient_id in range(1, num_patients + 1):\n",
        "        # Randomly generate a number of time points for each patient\n",
        "        num_time_points = np.random.randint(5, max_time_points + 1)\n",
        "\n",
        "        # Generate increasingly spaced time points using a cumulative sum to ensure they are sorted\n",
        "        time_increments = np.random.randint(1, 10, size=num_time_points)  # Random time intervals between observations\n",
        "        times = np.cumsum(time_increments)  # Cumulative sum to ensure times are sorted\n",
        "\n",
        "        # Initialize measurements at the first time point\n",
        "        measurements = np.random.rand(num_features)  # Initial random values for each feature\n",
        "\n",
        "        # Record initial state\n",
        "        data.append([patient_id, times[0], *measurements])\n",
        "\n",
        "        # Generate measurements for subsequent time points\n",
        "        for time_idx in range(1, num_time_points):\n",
        "            new_measurements = []\n",
        "            time = times[time_idx]\n",
        "            time_prev = times[time_idx - 1]\n",
        "            time_diff = time - time_prev\n",
        "\n",
        "            for feature_idx in range(num_features):\n",
        "                # Calculate new feature value based on the given formula\n",
        "                new_value = (feature_idx + 1) * measurements[feature_idx] + time_diff\n",
        "                new_measurements.append(new_value)\n",
        "\n",
        "            # Update measurements for the next iteration\n",
        "            measurements = new_measurements\n",
        "\n",
        "            # Append the new measurements to the data list\n",
        "            data.append([patient_id, time, *measurements])\n",
        "\n",
        "    # Convert data to DataFrame\n",
        "    column_names = ['patient_id', 'time'] + [f'measurement_{i+1}' for i in range(num_features)]\n",
        "    df = pd.DataFrame(data, columns=column_names)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Generate a toy dataset\n",
        "num_patients = 5\n",
        "max_time_points = 10\n",
        "num_features = 3  # Number of biomedical features\n",
        "df = generate_dynamic_data(num_patients, max_time_points, num_features)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head(15))\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, feature_size, num_layers, num_heads, dropout_rate=0.1):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=feature_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dropout=dropout_rate,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.output_layer = nn.Linear(feature_size, feature_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        # src and tgt are the input and target sequences\n",
        "        # src_mask and tgt_mask are the padding and causal masks respectively\n",
        "        out = self.transformer(src, tgt, src_key_padding_mask=src_mask, tgt_mask=tgt_mask, memory_key_padding_mask=src_mask)\n",
        "        return self.output_layer(out)\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz, device=src.device)).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "#def train(transformer, data_loader, loss_fn, optimizer, epochs=10):\n",
        "\n",
        "\n",
        "# Example use case: Assume `data_loader` is your PyTorch DataLoader that yields batches of data.\n",
        "# This is a minimalistic and illustrative example. In practice, you'll need to define the dataset class,\n",
        "# and handle batching, and possibly GPU computations.\n",
        "feature_size = 3  # Same as measurement dimensions\n",
        "num_layers = 3\n",
        "num_heads = 3\n",
        "transformer = TimeSeriesTransformer(feature_size, num_layers, num_heads)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.01)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Mock data loader (replace this with actual data preparation)\n",
        "src = torch.rand(10, 20, feature_size)  # (batch_size, sequence_length, feature_size)\n",
        "tgt = torch.rand(10, 20, feature_size)  # Same shape as src for simplicity\n",
        "data_loader = [(src, tgt)]\n",
        "\n",
        "# Train the model\n",
        "transformer.train()\n",
        "for epoch in range(100):\n",
        "    for src, tgt in data_loader:\n",
        "        src_mask = None  # No source masking in this autoregressive task\n",
        "        tgt_input = tgt[:, :-1]  # Use all but the last token for input to predict the next token\n",
        "        tgt_output = tgt[:, 1:]  # Use all but the first token for the target output\n",
        "        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1))  # Mask for the input part\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = transformer(src, tgt_input, src_mask, tgt_mask)\n",
        "        loss = loss_fn(output, tgt_output)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lGtG1B-UpZrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}